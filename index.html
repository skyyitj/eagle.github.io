<!DOCTYPE html>
<html>

<head>

  <meta charset="utf-8">
  <meta name="description"
    content="Efficient Training of Generalizable Visuomotor Policies via Control-Aware Augmentation">
  <meta name="keywords" content="Zero-shot Generalization, Visuomotor Policies, Data Augmentation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Efficient Training of Generalizable Visuomotor Policies via Control-Aware Augmentation</title>


  <script>
    window.dataLayer = window.dataLayer || [];
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  <!-- <link href="./static/css/googlecss.css"
        rel="stylesheet"> -->
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="./static/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="./static/js/jquerymin351.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <style>
    .author-block {
        display: block;
    }

</style>

</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Efficient Training of Generalizable Visuomotor Policies via
              Control-Aware Augmentation </h1>
            <div class="is-size-5 publication-authors">
                <span class="team-name"><b>Beijing Institute of Technology <p style="font-size: 70%"></p></b></span>
                <span class="author-block">Yinuo Zhao<sup>1</sup>, Kun Wu<sup>2</sup> Tianjiao Yi<sup>1</sup>,
                Zhiyuan Xu<sup>3</sup>, Zhengping Che<sup>3</sup>,<span class="author-block"></span> Qinru Qiu<sup>2</sup>, Chi Harold Liu<sup>1</sup>,Jian Tang<sup>3</sup>
    
                   

                <div class="is-size-6 publication-authors"> 
                  <span class="author-block">
  
                  </span>
                 <br>
               
                <div class="is-size-5 publication-authors id=institute">
                  <sup>1</sup>Beijing Institute of Technology, 
                  <sup>2</sup>Syracuse University, 
                  <sup>3</sup>Beijing Innovation Center of Humanoid Robotics
  
                </div>
                 <br>           
                </div>
              </div>
              
            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2401.09258" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/2412.13877" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span> -->
                <!-- Code Link. -->
                <!-- <span class="link-block">
                  <a href="https://github.com/x-humanoid-robomind"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://zitd5je6f7j.feishu.cn/share/base/form/shrcnOF6Ww4BuRWWtxljfs0aQqh"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Data</span>
                  </a>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/teaser.mp4" type="video/mp4">
        </video> -->
        <div class="yush-div-center">
          <img src="./static/images/overview.jpg" class="img-responsive">
        </div>

        <h2 class="subtitle has-text-centered">
          We introduce <span style="font-weight: bold"> EAGLE</span>, an Efficient trAining framework for GeneraLizablE visuomotor policies.
        </h2>
      </div>
    </div>
  </section>

  <section class="section" id="single-task-1">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Improving generalization is one key challenge in Embodied AI,
              where obtaining large-scale datasets across diverse scenarios is
              costly. Traditional weak augmentations, such as cropping and flipping, are insufficient for improving a model’s performance in new
              environments. Existing data augmentation methods often disrupt
              task-relevant information in images, potentially degrading performance. To overcome these challenges, we introduce EAGLE—an
              Efficient trAining framework for GeneraLizablE visuomotor policies—that improves upon existing methods by: 1) enhancing generalization by applying augmentation only to control-related regions
              identified through a self-supervised control-aware mask; and 2)
              improving training stability and efficiency by distilling knowledge
              from an expert to a visuomotor student policy, which is then deployed to unseen environments without further fine-tuning. Comprehensive experiments on three domains—including the DMControl Generalization Benchmark (DMC-GB), the enhanced Robot Manipulation Distraction Benchmark (RMDB), and a long-sequential
              drawer-opening task—validate the effectiveness of our method.
          </div>
        </div>
      </div>
      <br>
      <br>
      <!--/ Abstract. -->

      <!-- Hardware Setup. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Introduction</h2>
            <p> End-to-end visuomotor policies learn low-level controls directly
              from high-dimensional visual inputs, yielding promising results in
              tasks like robot manipulation , autonomous navigation ,
              and locomotion. However, visuomotor policies heavily rely
              on visual inputs for decision-making and control, making them
              susceptible to performance degradation when faced with changes
              in background, distractors, or viewpoints. This deficiency cannot
              be mitigated through reinforcement nor imitation learning alone.
              One promising technique to reduce the impact of these visual
              discrepancies is Data Augmentation. Weak augmentations, like random cropping and flipping, consistently enhance
              generalization, but with modest improvements. In contrast, strong
              augmentations such as random conv and random overlay,
              boost generalization capabilities through significantly diversifying
              the data. Nevertheless, they can indiscriminately distort the entire
              observation space, disrupting the control-related environmental
              structures and dynamics captured in the data. This often compli-
              cates training and destabilizes both learning and testing phases.
              While previous research has focused on augmenting specific
              areas within the observation space, they are limited to identifying
              dynamic objects without considering their task relevance. Re-
              cently, vision foundation models like SAM have shown strong
              generalization abilities. But they still require fine-tuning or human-
              given priors to identify task-relevant regions in the observation
              space. Therefore, automatically identifying control-related pixels
              for generalizable visuomotor policies still remains challenging.
              To improve generalization ability, we propose EAGLE—an ef-
              ficient framework for generating generalizable visuomotor poli-
              cies. EAGLE consists of two modules: 1) A control-aware augmen-
              tation module, which identifies control-related pixels using self-
              supervised reconstruction, and 2) A privilege-guided distillation
              module, which extracts control knowledge from an expert agent trained with deep reinforcement learning. 
              This approach enables zero-shot deployment in unseen environments, requiring no additional labels or reward signals. We evaluate EAGLE’s zero-shot
              generalization ability on the DMC-GB . Experimental results
              demonstrate that EAGLE significantly improvements the general-
              ization ability against challenging visual changes   </p>

          </div>
        </div>
      </div>
      <br>
      <br>
      <!-- / Hardware Setup. -->


      <!-- RoboMIND Data Analysis. -->
  <section class="section" id="demo">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
            <h2 class="title is-3">method</h2>
            <div class="container is-max-desktop">
            <div class="hero-body">
              <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
                <source src="./static/videos/teaser.mp4" type="video/mp4">
              </video> -->
              <div class="yush-div-center">
                <img src="./static/images/context_aug_model.jpg" class="img-responsive" style="width: 75%; height: auto;">
              </div>
              <h2 class="subtitle has-text-centered">
                Control-aware data augmentation module.
              </h2>

              <div class="content has-text-justified">
                <p>
                  We introduce EAGLE, an efficient training framework for generalizable visuomotor policies. The overall goal of EAGLE is to learn
                  visuomotor policies that are invariant and capable of zero-shot
                  generalization. EAGLE consists of two simultaneously optimized
                  modules: a control-aware augmentation module and a privilegeguided distillation module. The former module retrieves temporal
                  data from the replay buffer and conducts a self-supervised reconstruction task, accompanied by three auxiliary losses, to identify
                  control-related pixels. The latter module augments the observation
                  input and distills knowledge from a pretrained DRL expert (which
                  processes only environment states) into the visuomotor student
                  network (which processes only image observations). After training
                  is completed, the visuomotor policy can be reliable deployed in
                  complex environments with visual variations, without the need for
                  fine-tuning or additional supervision.
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!-- / RoboMIND Data Analysis. -->
  </section>

  <section class="section" id="demo">
    <div class="container is-max-desktop">
      <!-- Experiment  -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Experiments</h2>
      <!-- Experiment  -->
        <p> Experiment settings. We evaluate EAGLE using the DMC-GB,
          which tests an agent’s generalization ability from simple to com
          plex environments (Easy and Hard) with background changes. In
          Hard settings, the background features real-world videos that differ
          significantly from the training environment. Each method under
          goes 500k training iterations, with evaluations based only on visual
          inputs. We compare EAGLE to several SOTA algorithms, including
          SVEA , TLDA , VAI, SGQN in generalization abil
          ity. Besides, we develope a strong baseline SAM+E that combined
          SAM with our privilege expert.
          Comparison results. 

        </p>
        <p> 
          As shown in Tab. 1, EAGLE achieves an
          average return of 761 in Hard settings, which is 17.6% higher than
          previous state-of-the-art method SGQN. EAGLE overcomes visual
          distraction limitations via control-aware masks that preserves task
          critical regions while augmenting all irrelevant areas.
          Ablation studies.
          We investigate the effect of the proposed
          control-aware augmentation and privilege-guided distillation modules on training and generalization performance in Tab. 2. We can
          observe that indiscriminate use of strong augmentations degrades
          training performance, with Q+Aug achieving 160 lower average
          returns than Q-only. Adding the mask or the Expert alone can boost
          performance, with Q+Mask and E+Aug improving training results
          by 12% and 32%, respectively, and achieving 93% and 126% gains in
          Easy settings. In Hard settings, EAGLE achieves an average return
          of 761, with a 50% and 73% improvement over Q+Maks and E+Aug,
          respectively. This underscores the joint effect of two modules in
          enhancing the efficient generalization of visuomotor policies.
        </p>


        <div class="yush-div-center">
          <img src="./static/images/dmcgb_table.jpg" class="img-responsive">
        </div>

      <br>

      <div id="single-task">
        <!-- Performance on Single Tasks.. -->
         <br>
        <!-- <h3 class="title is-4">Success Examples of ACT on Single Tasks</h3>  -->
        <!-- <p> <b>Single Task Results</b>. ACT achieves an average success rate of 30.7% (Franka), 34.0% (Tien Kung), 55.3% (AgileX) and 38.0% (UR-5e).
        </p> -->
        <!-- <video width="640" height="360" controls>
          <source src="your-video.mp4" type="video/mp4">
      </video> -->

        <div class="content has-text-justified">
            <div class="columns">
              <div class="column has-text-centered">
                  <p style="font-size: 125%"><b>video1</b></p>
                  <video poster="" id="" autoplay="" controls="" muted="" loop="" height="100%" playbackrate="2.0" style="border-radius: 5px;">
                      <source src="./static/videos/video1.mp4" type="video/mp4">
                  </video>
              </div>
              <div class="column has-text-centered">
                  <p style="font-size: 125%"><b>video2</b></p>
                  <video poster="" id="" autoplay="" controls="" muted="" loop="" height="100%" playbackrate="2.0" style="border-radius: 5px;">
                      <source src="./static/videos/video2.mp4" type="video/mp4">
                  </video>
              </div>
              <div class="column has-text-centered">
                  <p style="font-size: 125%"><b>video3</b></p>
                  <video poster="" id="" autoplay="" controls="" muted="" loop="" height="100%" playbackrate="2.0" style="border-radius: 5px;">
                      <source src="./static/videos/video3.mp4" type="video/mp4">
                  </video>
              </div>
              <div class="column has-text-centered">
                <p style="font-size: 125%"><b>video4</b></p>
                <video poster="" id="" autoplay="" controls="" muted="" loop="" height="100%" playbackrate="2.0" style="border-radius: 5px;">
                    <source src="./static/videos/video4.mp4" type="video/mp4">
                </video>
            </div>


          </div>
         
        </div>

      </div>
      <br>
  </section>

  <section class="section" id="demo">
    <div class="container is-max-desktop">
      <!-- Experiment  -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Conclusion</h2>
      <!-- Experiment  -->
      <p> In this paper, we address the generalization challenge of visuomotor policies in the face of visual changes. We propose an Efficient
        trAining framework for GeneraLizablE visuomotor policies (EAGLE)
        designed to identify control-related regions and facilitate zero-shot
        generalization to unseen environments. EAGLE comprises two
        jointly optimized modules: a control-aware augmentation module and a privilege-guided policy distillation module. The former
        leverages a self-supervised reconstruction task with three auxiliary
        losses to learn a control-aware attention mask, which distinguishes
        task-irrelevant pixels and applies strong augmentations to minimize
        generalization gaps. The latter distills knowledge from a pretrained
        privileged expert into the visuomotor policies. We conduct extensive comparative and ablation studies across three challenging benchmarks to assess the efficacy of EAGLE. The experimental
        results well validate the effectiveness of our approach.
      </p>
      <br>

      <div id="single-task">
        <!-- Performance on Single Tasks.. -->
         <br>
        <!-- <h3 class="title is-4">Success Examples of ACT on Single Tasks</h3>  -->
        <!-- <p> <b>Single Task Results</b>. ACT achieves an average success rate of 30.7% (Franka), 34.0% (Tien Kung), 55.3% (AgileX) and 38.0% (UR-5e).
        </p> -->
        <!-- <video width="640" height="360" controls>
          <source src="your-video.mp4" type="video/mp4">
      </video> -->
      </div>
      <br>



  </section>



  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="https://arxiv.org/abs/2401.09258">
          <i class="fas fa-file-pdf"></i>
        </a>
        <!-- <a class="icon-link" href="https://x-humanoid-robomind.github.io/" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a> -->
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content has-text-centered">
            <p>
              The website template was borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>
              and <a href="https://eureka-research.github.io/">Eureka</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
